{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd15f13-c133-4773-98d8-fe99fe0893c0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fca92e-1899-4a3c-8a8c-295663d49151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42de7624-bad4-49a6-adab-799312cb5784",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb4223-231f-4687-bacc-ef51e5bceb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_data(df):\n",
    "    #Joining clock seconds and minutes\n",
    "    df['TimeLeft'] = (df['Clock Minutes']*60+df['Clock Seconds'])\n",
    "    df = df.drop('Clock Minutes', axis = 1)\n",
    "    df = df.drop('Clock Seconds', axis = 1)\n",
    "    \n",
    "    #Combining offense and defense score information\n",
    "    df['dScore'] = df['DefenseScore']-df['OffenseScore']\n",
    "    df = df.drop('DefenseScore', axis = 1)\n",
    "    df = df.drop('OffenseScore', axis = 1)\n",
    "    \n",
    "    #Consolidating play types\n",
    "    df.replace(to_replace='Pass Incomplete', value='Pass', inplace=True)\n",
    "    df.replace(to_replace='Pass Reception', value='Pass', inplace=True)\n",
    "    df.replace(to_replace='Passing Touchdown', value='Pass', inplace=True)\n",
    "    df.replace(to_replace='Sack', value='Pass', inplace=True)\n",
    "    \n",
    "    df.replace(to_replace='Field Goal Good', value='FG', inplace=True)\n",
    "    df.replace(to_replace='Field Goal Missed', value='FG', inplace=True)\n",
    "\n",
    "    df.replace(to_replace='Rushing Touchdown', value='Rush', inplace=True)\n",
    "    \n",
    "    Pass = np.array((df['PlayType'] == 'Pass').astype(int))\n",
    "    Rush = np.array((df['PlayType'] == 'Rush').astype(int))\n",
    "    Punt = np.array((df['PlayType'] == 'Punt').astype(int))\n",
    "    FG = np.array((df['PlayType'] == 'FG').astype(int))\n",
    "\n",
    "    #Adding in binary variable for later addition of previous play type\n",
    "    df['Pass'] = Pass\n",
    "\n",
    "    #Discarding non plays (penalties, timeouts, end of period)\n",
    "    keep_these = np.maximum(Pass, Rush)\n",
    "    keep_these = np.maximum(keep_these, Punt)\n",
    "    keep_these = np.maximum(keep_these, FG)\n",
    "    df = df.loc[np.where(keep_these == 1)[0]]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "    \n",
    "def grab_previous_plays(df):\n",
    "    #Loop to grab information on previous plays and cumulative scoring\n",
    "    new_arr = []\n",
    "    pass_prop_list = []\n",
    "    run_count = 0\n",
    "    pass_count = 0\n",
    "    play_count = 0\n",
    "    for i in range(max(df['DriveNumber'])+1):\n",
    "        drive_arr= df[df['DriveNumber'] == i].to_numpy()\n",
    "        if drive_arr.shape[0] != 0:\n",
    "            if drive_arr.shape[0] == 1:\n",
    "                if drive_arr[0][8] == 'Pass':\n",
    "                    pass_count += 1\n",
    "                elif drive_arr[0][8] == 'Rush':\n",
    "                    run_count += 1\n",
    "                if run_count+pass_count > 0:\n",
    "                    pass_prop = pass_count/(run_count+pass_count)\n",
    "                else: \n",
    "                    pass_prop = 0\n",
    "                play_count +=1\n",
    "                pass_prop_list.append(pass_prop)\n",
    "                if play_count > 0:\n",
    "                    new_arr.append(np.hstack((drive_arr[0],  np.array([0,0,0.5,0, 0.5, pass_prop_list[play_count-1]]))))\n",
    "                else:\n",
    "                    new_arr.append(np.hstack((drive_arr[0],  np.array([0,0,0.5,0, 0.5, 0.5]))))\n",
    "\n",
    "            else:\n",
    "                drive_arr = drive_arr[drive_arr[:,1].argsort()]\n",
    "                new_drive_arr = []\n",
    "                for j in range(len(drive_arr)):\n",
    "                    \n",
    "                    if drive_arr[j][8] == 'Pass':\n",
    "                        pass_count += 1\n",
    "                    elif drive_arr[j][8] == 'Rush':\n",
    "                        run_count += 1\n",
    "\n",
    "                    if run_count+pass_count > 0:\n",
    "                        pass_prop = pass_count/(run_count+pass_count)\n",
    "                    else: \n",
    "                        pass_prop = 0\n",
    "                    pass_prop_list.append(pass_prop)\n",
    "                    play_count+=1\n",
    "                    if j == 0:\n",
    "                        if play_count > 0:\n",
    "                            new_drive_arr.append(np.hstack((drive_arr[j], \n",
    "                                    np.array([0,0,0.5,0, 0.5, pass_prop_list[play_count-1]]))))\n",
    "                        else:\n",
    "                            new_drive_arr.append(np.hstack((drive_arr[j], \n",
    "                                    np.array([0,0,0.5,0, 0.5, 0.5]))))\n",
    "                        \n",
    "                    else:\n",
    "        \n",
    "                        prev_arr = new_drive_arr[j-1]\n",
    "                        current_play = drive_arr[j][8]\n",
    "                        \n",
    "                        if prev_arr[11] == 1:\n",
    "                            pos_3 = (prev_arr[14]*prev_arr[15]) + 1\n",
    "                            #prev play was pass\n",
    "                            if current_play == 'Pass':\n",
    "                                #current_play is a pass\n",
    "                                new_drive_arr.append(np.hstack((drive_arr[j],\n",
    "                                        np.array([prev_arr[5],prev_arr[7],pos_3, 1, prev_arr[11], pass_prop_list[play_count-1]]))))\n",
    "                            else:\n",
    "                                new_drive_arr.append(np.hstack((drive_arr[j], \n",
    "                                        np.array([prev_arr[5],prev_arr[7],pos_3,0, prev_arr[11], pass_prop_list[play_count-1]]))))\n",
    "                        elif prev_arr[11] == 0:\n",
    "                            pos_3 = (prev_arr[14]*prev_arr[15]) - 1\n",
    "                            #prev play was rush\n",
    "                            if current_play == 'Rush':\n",
    "                                #current_play is a rush\n",
    "                                new_drive_arr.append(np.hstack((drive_arr[j], \n",
    "                                    np.array([prev_arr[5],prev_arr[7],pos_3, 1, prev_arr[11], pass_prop_list[play_count-1]]))))\n",
    "                            else:\n",
    "                                new_drive_arr.append(np.hstack((drive_arr[j], \n",
    "                                    np.array([prev_arr[5],prev_arr[7],pos_3, 0, prev_arr[11], pass_prop_list[play_count-1]]))))\n",
    "                new_arr.append(np.vstack(new_drive_arr))    \n",
    "\n",
    "    #discard data that is not helpful\n",
    "    full_arr = np.vstack(new_arr)\n",
    "    full_arr = np.delete(full_arr, np.array([0, 1, 2, 11, 15]), axis = 1)\n",
    "    \n",
    "    return full_arr\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = self.X.shape[0]\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c67d9-f048-4841-b735-8009ef1b262b",
   "metadata": {},
   "source": [
    "# Grabbing and formatting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4469f47-43c1-45fa-8058-17c40913378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path should be a folder containing all team data labeled as team_game_year\n",
    "Folder_Path = \"\"\n",
    "\n",
    "csv_files = sorted(glob.glob(os.path.join(Folder_Path, \"*.csv\")))\n",
    "df_dict = {}\n",
    "for f in csv_files:\n",
    "    year = int(f.split('\\\\')[-1].split('_')[-1].split('.')[0])\n",
    "    team = int(f.split('\\\\')[-1].split('_')[0])\n",
    "    game = int(f.split('\\\\')[-1].split('_')[1])\n",
    "    df = pd.read_csv(f, sep = ',')\n",
    "    df_dict[game, team] = df[['DriveNumber', 'PlayNumber', 'OffenseScore', 'DefenseScore', 'Home', 'Period', 'Clock Minutes', 'Clock Seconds', \n",
    " 'YardsToGoal', 'Down', 'Distance', 'YardsGained', 'PlayType']]\n",
    "\n",
    "# The following list of pass % was manually input for each team in 2023 from ESPN datqa. The ith position corresponding to the ith team. \n",
    "percent_pass = [0.42, 0.57, 0.49, 0.51, 0.46, 0.36, 0.46, 0.48, 0.44, 0.56, 0.51, 0.42, 0.40]\n",
    "\n",
    "#reformatting and combining the data into an array\n",
    "all_data = []\n",
    "for key in df_dict.keys():\n",
    "    df = reformat_data(df_dict[key])\n",
    "    data_arr = grab_previous_plays(df)\n",
    "    data_arr = np.vstack((data_arr.T, np.full(len(data_arr), percent_pass[key[1]-1]))).T\n",
    "    all_data.append(data_arr)\n",
    "game_data = np.vstack(all_data)\n",
    "\n",
    "#play type incoder tranforming each play into an array [0,0,0,0] with a 1 in the position corresponding to the play call\n",
    "encoder = sklearn.preprocessing.OneHotEncoder(sparse_output = False).fit(game_data[:,5].astype(str).reshape(-1,1))\n",
    "encoded_output = encoder.transform(game_data[:,5].astype(str).reshape(-1,1))\n",
    "\n",
    "#feature selection of data that does not inform post-snap information\n",
    "game_data = game_data[:,[0,1,2,3,7,8,9,10,11,12,13]]\n",
    "\n",
    "#The data is scaled between 0 and 1\n",
    "scaled_data = []\n",
    "for i in range(len(game_data.T)):\n",
    "    max_val = max(game_data[:,i])\n",
    "    min_val = min(game_data[:,i])\n",
    "    scaled = ((game_data[:,i]-min_val)/(max_val-min_val))\n",
    "    scaled_data.append(scaled)\n",
    "game_data = np.vstack(scaled_data).T.astype('float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567e7e7-db4d-4415-878a-56c29a3b6646",
   "metadata": {},
   "source": [
    "# Loading the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d908313a-02f0-4980-a1b6-23e6ae3a1f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split up data into test and training\n",
    "play_type = encoded_output\n",
    "Pre_snap_data = game_data\n",
    "data_train, data_test, play_type_train, play_type_test = train_test_split(Pre_snap_data, play_type, train_size=0.8, shuffle=True)\n",
    "\n",
    "#Convert the data to PyTorch Tensors\n",
    "data_train = torch.tensor(data_train, dtype=torch.float32)\n",
    "play_type_train = torch.tensor(play_type_train, dtype=torch.float32)\n",
    "data_test = torch.tensor(data_test, dtype=torch.float32)\n",
    "play_type_test = torch.tensor(play_type_test, dtype=torch.float32)\n",
    "\n",
    "#Setting batch size and putting the data into the dataloader\n",
    "batch_size = 32\n",
    "\n",
    "train_data = Data(data_train, play_type_train)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "test_data = Data(data_test, play_type_test)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9940c640-9599-4363-b2d3-6225549a7f58",
   "metadata": {},
   "source": [
    "# Defining the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92af8619-7316-4a6c-a67d-6076e0acda22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(11, 165)\n",
    "        self.linear2 = nn.Linear(165, 153)\n",
    "        self.linear3 = nn.Linear(153, 68)\n",
    "        self.linear4 = nn.Linear(68, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deceb701-d83c-4a14-981d-6d0f03294e37",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5e5d2-c2df-4e05-a0cb-41d482313fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.009084)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "loss_vals = []\n",
    "loss = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in train_dataloader:\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_vals.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ab6a7-efc5-4cc7-9e08-eb3a197ff0a0",
   "metadata": {},
   "source": [
    "## Plotting Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3fd759-00d2-4641-a2f8-039212286124",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = range(len(loss_vals))\n",
    "div_val = max(step)/num_epochs\n",
    "\n",
    "plt.plot(np.array(step)/div_val, np.array(loss_vals), color = 'red')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec6fa8-15e2-4771-9fa7-7d370995194a",
   "metadata": {},
   "source": [
    "# Validating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac0269e-06d8-4367-9550-44cc48fa5536",
   "metadata": {},
   "source": [
    "#### Note: The code below is used for the validation data, use the code below for running test data as well once hyperparameters are tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f79028-7135-4303-b470-772fdf83fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = []\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        pred= model(X)  # Get model outputs\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        pred_int = torch.max(pred, axis = 1)[1]\n",
    "        gt_int = torch.max(y, axis = 1)[1]\n",
    "\n",
    "        acc_list.append(np.vstack([pred_int, gt_int]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da74b0d-852c-4a14-9374-446aacce02fb",
   "metadata": {},
   "source": [
    "## Plotting the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c840a-7229-4836-9934-4f9cbfa51760",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_arr = np.vstack(acc_list)\n",
    "print('Accuracy:', sum(acc_arr[:,0]==acc_arr[:,1])/(len(acc_arr)),'%')\n",
    "\n",
    "matrix = confusion_matrix(acc_arr[:,1], acc_arr[:,0])\n",
    "summed = np.sum(matrix, axis = 1)\n",
    "summed_div = np.vstack((summed, summed, summed, summed)).T\n",
    "\n",
    "plt.subplots(figsize=(8, 5))\n",
    "ticklabels = encoder.categories_[0]\n",
    "ax1 = sns.heatmap(matrix/summed_div, annot=matrix, xticklabels = ticklabels, yticklabels = ticklabels,  cbar=False, linewidth = 2, linecolor = 'white', cmap = 'gist_heat', vmin = 0, vmax = 2, fmt=\"g\")\n",
    "# cbar = ax1.collections[0].colorbar\n",
    "# cbar.ax.tick_params(colors = 'black')\n",
    "# cbar.ax.set_yticks(ticks = [0,2])\n",
    "# cbar.ax.set_yticklabels(labels = ['Min','Max'], fontsize = 14)\n",
    "plt.title('Confusion Matrix', fontsize = 18)\n",
    "plt.xlabel('Predicted', fontsize = 14)\n",
    "plt.ylabel('Ground Truth', fontsize = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db8c060-c919-4d45-a58a-965a371a8bd3",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec8904-805f-4ab7-adbc-c46ba7625f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c18c6-f5d5-41ef-afcc-9d7c19c4a2e2",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af68dd35-58e3-47cf-9a99-85e8bdf7d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_hyperparams(\n",
    "    H_Layers: Optional[float] = None, \n",
    "    Hidden_layer_1_Nodes: Optional[float] = None, \n",
    "    Hidden_layer_2_Nodes: Optional[float] = None, \n",
    "    Hidden_layer_3_Nodes: Optional[float] = None, \n",
    "    learning_rate: Optional[float] = None, \n",
    "    batch_size: Optional[float] = None\n",
    "    ) -> float:\n",
    "    \n",
    "    global Pre_snap_data\n",
    "    global play_type\n",
    "\n",
    "    #Reformating input variables\n",
    "    Hidden_layer_1_Nodes = int(np.round(Hidden_layer_1_Nodes))\n",
    "    Hidden_layer_2_Nodes = int(np.round(Hidden_layer_2_Nodes))\n",
    "    Hidden_layer_3_Nodes = int(np.round(Hidden_layer_3_Nodes))\n",
    "\n",
    "    #Defining batch size\n",
    "    batch_size = 32    \n",
    "\n",
    "    #Creating the model\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(11, Hidden_layer_1_Nodes),\n",
    "        nn.ReLU(), \n",
    "        nn.Linear(Hidden_layer_1_Nodes, Hidden_layer_2_Nodes),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(Hidden_layer_2_Nodes, Hidden_layer_3_Nodes),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(Hidden_layer_3_Nodes, 4),\n",
    "    )\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #Looping thought the model to average validation output, resplitting the data each time.\n",
    "    Epochs = 10\n",
    "    output = []\n",
    "    for i in range(5):\n",
    "        data_train, data_test, play_type_train, play_type_test = train_test_split(Pre_snap_data, play_type, train_size=0.8, shuffle=True)\n",
    "        \n",
    "        data_train = torch.tensor(data_train, dtype=torch.float32)\n",
    "        play_type_train = torch.tensor(play_type_train, dtype=torch.float32)\n",
    "        data_test = torch.tensor(data_test, dtype=torch.float32)\n",
    "        play_type_test = torch.tensor(play_type_test, dtype=torch.float32)\n",
    "        \n",
    "        train_data = Data(data_train, play_type_train)\n",
    "        train_dataloader = DataLoader(dataset=train_data, batch_size = batch_size, shuffle=True)\n",
    "        test_data = Data(data_test, play_type_test)\n",
    "        test_dataloader = DataLoader(dataset=test_data, batch_size = batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in range(Epochs):\n",
    "            for X, y in train_dataloader:\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        acc_list = []\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_dataloader:\n",
    "                pred= model(X)  # Get model outputs\n",
    "                loss = loss_fn(pred, y)\n",
    "        \n",
    "                pred_int = torch.max(pred, axis = 1)[1]\n",
    "                gt_int = torch.max(y, axis = 1)[1]\n",
    "        \n",
    "                acc_list.append([pred_int, gt_int])\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for j in range(len(acc_list)):\n",
    "            correct += sum(acc_list[j][0]==acc_list[j][1])\n",
    "            total += len(acc_list[j][0])\n",
    "        output.append(correct/total)\n",
    "    \n",
    "    return np.average(np.vstack((output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7034069-c50e-4d91-be65-bd1406da934f",
   "metadata": {},
   "source": [
    "### Running the Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee34ea-6615-4412-afa8-d88fa3084c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {'Hidden_layer_1_Nodes': (2, 200), 'Hidden_layer_2_Nodes': (2, 200), 'Hidden_layer_3_Nodes': (2, 200), 'learning_rate': (0.0005, 0.1)}\n",
    "\n",
    "runs = 3\n",
    "rand_n = 1\n",
    "\n",
    "optimizer = BayesianOptimization(f = NN_hyperparams, \n",
    "                                    pbounds=pbounds, verbose=2, \n",
    "                                    random_state=int(time.time()),\n",
    "                                    allow_duplicate_points=True)\n",
    "\n",
    "optimizer.maximize(init_points=runs, n_iter=rand_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9e0f9-0de3-4c50-88b0-155eccf1ff08",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39761ced-779a-4787-b293-21ba33ad6697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabbing the optimizer data\n",
    "LR = np.array([[res[\"params\"][\"learning_rate\"]] for res in optimizer.res]).flatten()\n",
    "HL1 = np.array([[res[\"params\"][\"Hidden_layer_1_Nodes\"]] for res in optimizer.res]).flatten()\n",
    "HL2 = np.array([[res[\"params\"][\"Hidden_layer_2_Nodes\"]] for res in optimizer.res]).flatten()\n",
    "HL3 = np.array([[res[\"params\"][\"Hidden_layer_3_Nodes\"]] for res in optimizer.res]).flatten()\n",
    "LR = np.array([[res[\"params\"][\"learning_rate\"]] for res in optimizer.res]).flatten()\n",
    "target_arr = np.array([[res[\"target\"] for res in optimizer.res]]).flatten()\n",
    "opt_arr = np.vstack((HL1, HL2, HL3, LR, target_arr)).T\n",
    "\n",
    "df = pd.DataFrame(opt_arr, columns = ['Hidden Neurons 1', 'Hidden Neurons 2', 'Hidden Neurons 3', 'Learning Rate', 'Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa52131-1d95-422d-807a-811f0fb57cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting with plotly\n",
    "fig = go.Figure(data=\n",
    "    go.Parcoords(\n",
    "        line = dict(color = df['Accuracy'],\n",
    "                   colorscale = 'Reds',\n",
    "                   showscale = True,\n",
    "                   cmin = min(df['Accuracy']),\n",
    "                   cmax = max(df['Accuracy'])),\n",
    "        dimensions = list([\n",
    "            dict(range = [1,200],\n",
    "                 label = 'Hidden Neurons 1', values = df['Hidden Neurons 1']),\n",
    "            dict(range = [1,200],\n",
    "                 label = 'Hidden Neurons 2', values = df['Hidden Neurons 2']),\n",
    "            dict(range = [1,200],\n",
    "                 label = 'Hidden Neurons 3', values = df['Hidden Neurons 3']),\n",
    "            dict(range = [min(df['Learning Rate']),max(df['Learning Rate'])],\n",
    "                 label = 'Learning Rate', values = df['Learning Rate']),\n",
    "            dict(range = [min(df['Accuracy']),max(df['Accuracy'])],\n",
    "                 label = 'Accuracy', values = df['Accuracy']),\n",
    "            ])\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    font = dict(size = 20)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978148a8-bffa-4a87-b81e-544e0d4986f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
